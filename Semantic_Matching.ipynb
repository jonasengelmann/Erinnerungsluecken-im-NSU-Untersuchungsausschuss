{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "orig_nbformat": 2,
    "file_extension": ".py",
    "mimetype": "text/x-python",
    "name": "python",
    "npconvert_exporter": "python",
    "pygments_lexer": "ipython3",
    "version": 3,
    "colab": {
      "name": "semantic_matching.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2D8lx2iwtro",
        "colab_type": "text"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jonasengelmann/erinnerungsluecken-im-nsu-untersuchungsausschuss/blob/master/Semantic_Matching.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9LkKR_A7wzQy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import re\n",
        "import string\n",
        "import subprocess\n",
        "import pickle\n",
        "import urllib.request\n",
        "import xml.etree.ElementTree as ET\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "pd.set_option('max_colwidth', 100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9zT1D-3VqFog",
        "colab": {}
      },
      "source": [
        "# Create folder structure:\n",
        "pdf_folder = Path.cwd() / '01_data' / '01_pdf'\n",
        "Path.mkdir(pdf_folder, parents=True, exist_ok=True)\n",
        "\n",
        "xml_folder = pdf_folder.parent / '02_xml'\n",
        "Path.mkdir(xml_folder, exist_ok=True)\n",
        "\n",
        "results_folder = pdf_folder.parent.parent / '02_results'\n",
        "Path.mkdir(results_folder, exist_ok=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yiyZpdZJqFoi"
      },
      "source": [
        "You can either download and scrape the pdfs, or use the provided files in the data folder\n",
        "of the repository and skip the next two cells!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Wl7oJGhBqFoj"
      },
      "source": [
        "#1 Download transcripts as PDFs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "W55Y8ZcCqFok"
      },
      "source": [
        "I collected urls to the transcriptions containing witness interrogations in a list. Let's download them!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dBQ3kTFqqFok",
        "colab": {}
      },
      "source": [
        "urls = ['http://dipbt.bundestag.de/doc/btd/17/CD14600/Protokolle/Protokoll-Nr%2012.pdf',\n",
        "        'http://dipbt.bundestag.de/doc/btd/17/CD14600/Protokolle/Protokoll-Nr%2014.pdf',\n",
        "        'http://dipbt.bundestag.de/doc/btd/17/CD14600/Protokolle/Protokoll-Nr%2015.pdf',\n",
        "        'http://dipbt.bundestag.de/doc/btd/17/CD14600/Protokolle/Protokoll-Nr%2017.pdf',\n",
        "        'http://dipbt.bundestag.de/doc/btd/17/CD14600/Protokolle/Protokoll-Nr%2019.pdf',\n",
        "        'http://dipbt.bundestag.de/doc/btd/17/CD14600/Protokolle/Protokoll-Nr%2021.pdf',\n",
        "        'http://dipbt.bundestag.de/doc/btd/17/CD14600/Protokolle/Protokoll-Nr%2022a.pdf',\n",
        "        'http://dipbt.bundestag.de/doc/btd/17/CD14600/Protokolle/Protokoll-Nr%2022b.pdf',\n",
        "        'http://dipbt.bundestag.de/doc/btd/17/CD14600/Protokolle/Protokoll-Nr%2024a.pdf',\n",
        "        'http://dipbt.bundestag.de/doc/btd/17/CD14600/Protokolle/Protokoll-Nr%2024b.pdf',\n",
        "        'http://dipbt.bundestag.de/doc/btd/17/CD14600/Protokolle/Protokoll-Nr%2027.pdf',\n",
        "        'http://dipbt.bundestag.de/doc/btd/17/CD14600/Protokolle/Protokoll-Nr%2029a.pdf',\n",
        "        'http://dipbt.bundestag.de/doc/btd/17/CD14600/Protokolle/Protokoll-Nr%2029b.pdf',\n",
        "        'http://dipbt.bundestag.de/doc/btd/17/CD14600/Protokolle/Protokoll-Nr%2031.pdf',\n",
        "        'http://dipbt.bundestag.de/doc/btd/17/CD14600/Protokolle/Protokoll-Nr%2032.pdf',\n",
        "        'http://dipbt.bundestag.de/doc/btd/17/CD14600/Protokolle/Protokoll-Nr%2034a.pdf',\n",
        "        'http://dipbt.bundestag.de/doc/btd/17/CD14600/Protokolle/Protokoll-Nr%2034b.pdf',\n",
        "        'http://dipbt.bundestag.de/doc/btd/17/CD14600/Protokolle/Protokoll-Nr%2036.pdf',\n",
        "        'http://dipbt.bundestag.de/doc/btd/17/CD14600/Protokolle/Protokoll-Nr%2039.pdf',\n",
        "        'http://dipbt.bundestag.de/doc/btd/17/CD14600/Protokolle/Protokoll-Nr%2041.pdf',\n",
        "        'http://dipbt.bundestag.de/doc/btd/17/CD14600/Protokolle/Protokoll-Nr%2043.pdf',\n",
        "        'http://dipbt.bundestag.de/doc/btd/17/CD14600/Protokolle/Protokoll-Nr%2044.pdf',\n",
        "        'http://dipbt.bundestag.de/doc/btd/17/CD14600/Protokolle/Protokoll-Nr%2047.pdf',\n",
        "        'http://dipbt.bundestag.de/doc/btd/17/CD14600/Protokolle/Protokoll-Nr%2049a.pdf',\n",
        "        'http://dipbt.bundestag.de/doc/btd/17/CD14600/Protokolle/Protokoll-Nr%2049b.pdf',\n",
        "        'http://dipbt.bundestag.de/doc/btd/17/CD14600/Protokolle/Protokoll-Nr%2051.pdf',\n",
        "        'http://dipbt.bundestag.de/doc/btd/17/CD14600/Protokolle/Protokoll-Nr%2053.pdf',\n",
        "        'http://dipbt.bundestag.de/doc/btd/17/CD14600/Protokolle/Protokoll-Nr%2054.pdf',\n",
        "        'http://dipbt.bundestag.de/doc/btd/17/CD14600/Protokolle/Protokoll-Nr%2056a.pdf',\n",
        "        'http://dipbt.bundestag.de/doc/btd/17/CD14600/Protokolle/Protokoll-Nr%2056b.pdf',\n",
        "        'http://dipbt.bundestag.de/doc/btd/17/CD14600/Protokolle/Protokoll-Nr%2057.pdf',\n",
        "        'http://dipbt.bundestag.de/doc/btd/17/CD14600/Protokolle/Protokoll-Nr%2059a.pdf',\n",
        "        'http://dipbt.bundestag.de/doc/btd/17/CD14600/Protokolle/Protokoll-Nr%2059b.pdf',\n",
        "        'http://dipbt.bundestag.de/doc/btd/17/CD14600/Protokolle/Protokoll-Nr%2060.pdf',\n",
        "        'http://dipbt.bundestag.de/doc/btd/17/CD14600/Protokolle/Protokoll-Nr%2062.pdf',\n",
        "        'http://dipbt.bundestag.de/doc/btd/17/CD14600/Protokolle/Protokoll-Nr%2064a.pdf',\n",
        "        'http://dipbt.bundestag.de/doc/btd/17/CD14600/Protokolle/Protokoll-Nr%2064b.pdf',\n",
        "        'http://dipbt.bundestag.de/doc/btd/17/CD14600/Protokolle/Protokoll-Nr%2065.pdf',\n",
        "        'http://dipbt.bundestag.de/doc/btd/17/CD14600/Protokolle/Protokoll-Nr%2066a.pdf',\n",
        "        'http://dipbt.bundestag.de/doc/btd/17/CD14600/Protokolle/Protokoll-Nr%2066b.pdf',\n",
        "        'http://dipbt.bundestag.de/doc/btd/17/CD14600/Protokolle/Protokoll-Nr%2068a.pdf',\n",
        "        'http://dipbt.bundestag.de/doc/btd/17/CD14600/Protokolle/Protokoll-Nr%2068b.pdf',\n",
        "        'http://dipbt.bundestag.de/doc/btd/17/CD14600/Protokolle/Protokoll-Nr%2070a.pdf',\n",
        "        'http://dipbt.bundestag.de/doc/btd/17/CD14600/Protokolle/Protokoll-Nr%2070b.pdf',\n",
        "        'http://dipbt.bundestag.de/doc/btd/17/CD14600/Protokolle/Protokoll-Nr%2072a.pdf',\n",
        "        'http://dipbt.bundestag.de/doc/btd/17/CD14600/Protokolle/Protokoll-Nr%2072b.pdf',\n",
        "        'http://dipbt.bundestag.de/doc/btd/17/CD14600/Protokolle/Protokoll-Nr%2074.pdf']\n",
        "\n",
        "for url in urls:\n",
        "    print(f'Downloading {url}')\n",
        "    urllib.request.urlretrieve(url, pdf_folder / url.split(\"/\")[-1])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "g2oY2jF1qFon"
      },
      "source": [
        "#2 Scraping with pdfminer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AW-W4-aEqFon"
      },
      "source": [
        "Lets scrape the pdfs using pdfminer!\n",
        "\n",
        "We want to preserve some of the layout information, such as font type and font size to later be able to easier differentiate between different speakers, remarks, quotations, etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EO-Y6Q5HrlCm",
        "colab": {}
      },
      "source": [
        "# This step requires pdfminer to be installed. \n",
        "# For python2 use: pip install pdfminer\n",
        "!pip install pdfminer.six"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Eci2Xc4_qFoo",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "\n",
        "# Scraping all pdfs might take a while! [It took me around 10mins]\n",
        "\n",
        "for filename in list(pdf_folder.glob('*.pdf')):\n",
        "    target = xml_folder / f'{filename.stem}.xml'\n",
        "    print(f'Scraping {filename}')\n",
        "    subprocess.call(['pdf2txt.py', '-t', 'xml', '-n', '-o', target, filename])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6Cyv2eP0qFos",
        "colab": {}
      },
      "source": [
        "%%time \n",
        "\n",
        "# Lets parse the generated xmls (this may take a while [2mins for me]):\n",
        "\n",
        "def crop_bottom_and_top(document, crop_val):\n",
        "    '''\n",
        "    Removes text elements at the top and bottom of \n",
        "    the document given by percentage of the page size.\n",
        "    '''\n",
        "    crop_document = []\n",
        "    for page in document:\n",
        "        crop_page = []\n",
        "        y1 = float(page.attrib[\"bbox\"].split(\",\")[3])\n",
        "        for char in page:\n",
        "            if (char.tag == \"text\" \n",
        "               and float(char.attrib['bbox'].split(\",\")[3]) > y1*crop_val/100\n",
        "               and float(char.attrib['bbox'].split(\",\")[3]) < y1*(1-crop_val/100)):\n",
        "                \n",
        "                crop_page.append(char)\n",
        "        crop_document.append(crop_page)\n",
        "    return crop_document\n",
        "\n",
        "def check_if_characters_match_style(n_characters, font, size):\n",
        "    '''\n",
        "    Checks if all characters match a specified font and\n",
        "    font size. n_characters has to be a list of text elements. \n",
        "    '''  \n",
        "    checks = []\n",
        "    for single_char in n_characters:\n",
        "        if ((font in single_char.attrib[\"font\"].lower()\n",
        "           and single_char.attrib[\"size\"].startswith(size))\n",
        "           or not single_char.text.strip()):\n",
        "            checks.append(True)\n",
        "    # Check if only empty characters:\n",
        "    if any(char.text.strip() for char in n_characters):\n",
        "        return len(checks) == len(n_characters)\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "def find_next_speaker_and_text(characters):\n",
        "    '''\n",
        "    Finds the next speaker and text on the basis that speaker are always\n",
        "    written with bold font and in font size 9.\n",
        "    '''\n",
        "    speaker, text = [], []\n",
        "    record_speaker = False\n",
        "    for idx, char in enumerate(characters):\n",
        "\n",
        "        # Check if next 10 characters are bold and in font size 9:\n",
        "        if (check_if_characters_match_style(characters[idx:idx+10], \"bold\", \"9\")\n",
        "           and not record_speaker):\n",
        "\n",
        "            yield \"\".join(speaker), clean_text(\"\".join(text))\n",
        "            \n",
        "            record_speaker = True\n",
        "            speaker = []\n",
        "\n",
        "        if record_speaker:\n",
        "            if char.attrib[\"size\"].strip().startswith(\"9\"):\n",
        "                speaker.append(char.text)\n",
        "        \n",
        "            # Check if it is the end of the speaker's name:\n",
        "            next_char = characters[idx+1] if (idx+1) != len(characters) else char \n",
        "            \n",
        "            if not \"bold\" in next_char.attrib[\"font\"].lower() and next_char.text.strip():\n",
        "                record_speaker = False\n",
        "                text = []\n",
        "\n",
        "        elif char.attrib[\"size\"].strip().startswith(\"9\"):\n",
        "                text.append(char.text)\n",
        "\n",
        "def clean_text(text):\n",
        "    '''\n",
        "    Removes multiple space and hyphens resulting from linebreaks.\n",
        "    '''\n",
        "    text = re.sub(' +', ' ', text)\n",
        "    return re.sub(r\"([a-zßäöü])-[ ]*([a-zßäöü])\", '\\\\1\\\\2', text)\n",
        "\n",
        "\n",
        "protocol = []\n",
        "for filename in sorted(list(xml_folder.glob('*.xml'))):\n",
        "\n",
        "    document = ET.parse(filename).getroot()\n",
        "\n",
        "    # Crop bottom and top by 7 procent to discard of headers and footers\n",
        "    document = crop_bottom_and_top(document, 7)\n",
        "\n",
        "    # Collect all text characters\n",
        "    characters = [char for page in document for char in page if char.tag == \"text\"]\n",
        "\n",
        "    # Parse content of xmls\n",
        "    for speaker, text in find_next_speaker_and_text(characters):\n",
        "        if speaker.strip():\n",
        "            protocol.append((speaker, text))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dA_eKWj8qFow",
        "colab": {}
      },
      "source": [
        "# Save extracted data to disk:\n",
        "\n",
        "with open(results_folder / 'parsed_dialog.txt', 'w') as output:\n",
        "    for speaker, text in protocol:\n",
        "        output.write(f'{speaker.strip()} {text.strip()}\\n\\n')\n",
        "    \n",
        "with open(results_folder / 'only_witnesses_text.txt', 'w') as output:\n",
        "    for speaker, text in protocol: \n",
        "        if 'zeug' in speaker.lower():\n",
        "            output.write(text + '. ')\n",
        "\n",
        "pickle.dump(protocol, open( results_folder / 'parsed_dialog.p', 'wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9SD98Up24AN0",
        "colab": {}
      },
      "source": [
        "# You can download the parsed_dialog file like this:\n",
        "from google.colab import files\n",
        "files.download(results_folder / 'parsed_dialog.txt')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bfDNO1vMqFoy"
      },
      "source": [
        "#3 Semantic Matching"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HHzYvmXYqFoy"
      },
      "source": [
        "Now let's try to identify instances in which the inabilty to remember is expressed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sJzIqLzgqFoz",
        "colab": {}
      },
      "source": [
        "sem_folder = results_folder.parent / '03_semantic_matching'\n",
        "Path.mkdir(sem_folder, exist_ok=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WEgQD8DLqFo2"
      },
      "source": [
        "## 3.1 With a simple regular expression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TZAE9O2zqFo2"
      },
      "source": [
        " Expressions of remembering really come down to a few words in German. A few nouns like memory (Erinnerung, Gedächtnis), a few verbs like remembering (sich erinnern, sich entsinnen), and some rather rare adjective (erinnerlich), etc. So let's try to match those in combination with a negation with a simple regular expression. We have to be careful with the scope of the regular expression as we do not want negating words in other parts of the sentence to be confused. As a simple window we can just try to match on half sentences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lYPhvBlsqFo3",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "def cant_remember_matcher(input_text):\n",
        "    '''\n",
        "    A simple regular expression that will match instances of not remembering.\n",
        "    '''\n",
        "    wildcards = r'[äöüßa-zÄÖÜA-Z- ]*'\n",
        "    # we only want to match relevant half sentences here to avoid confusions\n",
        "    # with negations in other parts of the same sentence:\n",
        "    borders = r'[,.;:?!]' \n",
        "    bw = borders + wildcards\n",
        "    wb = wildcards + borders\n",
        "\n",
        "    sem_matcher = [f'{bw}keine{wildcards}Erinnerung{wb}',\n",
        "                   f'{bw}nicht{wildcards}Erinnerung{wb}',\n",
        "                   f'{bw}Erinnerung{wildcards}nicht{wb}',\n",
        "                   f'{bw}weiß{wildcards}nicht{wildcards}mehr{wb}',\n",
        "                   f'{bw}nicht{wildcards}erinner{wb}',\n",
        "                   f'{bw}nicht{wildcards}entsinne{wb}',\n",
        "                   f'{bw}nicht{wildcards}Gedächtnis{wb}',\n",
        "                   f'{bw}Gedächtnis{wildcards}nicht{wb}',\n",
        "                   f'{bw}nicht{wildcards}mehr {wildcards}sagen{wb}',\n",
        "                   f'{bw}fällt{wildcards}nicht{wildcards}ein{wb}',\n",
        "                   f'{bw}nicht{wildcards}mehr{wildcards}rekonstruieren{wb}',\n",
        "                   f'{bw}nicht{wildcards}gegenwärtig{wb}',\n",
        "                   f'{bw}nicht{wildcards}präsent{wb}',\n",
        "                   f'{bw}nicht{wildcards}im Kopf{wb}',\n",
        "                   f'{bw}nicht{wildcards}im Hinterkopf{wb}']\n",
        "                   \n",
        "    return re.findall(\"|\".join(sem_matcher), text, re.IGNORECASE)\n",
        "                      \n",
        "erinnerungsluecken = []\n",
        "for speaker, text in protocol:\n",
        "    if \"zeug\" in speaker.lower():\n",
        "        result = cant_remember_matcher(text)\n",
        "        for match in result:\n",
        "            erinnerungsluecken.append(f'{speaker.strip()} ...{match[1:-1].strip()}... ')\n",
        "\n",
        "print(len(erinnerungsluecken))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Z44o20iBqFo6",
        "colab": {}
      },
      "source": [
        "# Save results to file:\n",
        "with open(results_folder / \"erinnerungsluecken.txt\", \"w\") as output:\n",
        "    for x in erinnerungsluecken:\n",
        "        output.write(x +'\\n')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "x5lEOoiLqFo8"
      },
      "source": [
        "Regular expression will only match the rules we defined, so it only takes us so far. How could we generalize our semantic matcher, in a way that other expressions with the same semantic meaning yet different words are matched as well?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MeEkGXIfqFo-"
      },
      "source": [
        "## 3.2 Text classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oWCPQzWNF4eh"
      },
      "source": [
        "One promising approach is to move from words to word embeddings. Word embedinngs encode words into vectors in a n-dimensional vector space. Ideally, words with similar meaning share a similar location in the vector space, so by 'widening' the space around one known word vector we could achieve some generalization.\n",
        "\n",
        "Further, we want to transform our task into a classification task. We could simply think of it as a 'spam' vs 'not spam' identification task, and swaping in 'expression of not remembering' vs 'anything else'. In order to do so, we need to first train a model. Fasttext is a simple tool, that let's us do all that fairly easy and really fast. Let's preprocess our data and see how well it will perform."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rfpuul-HE6t_"
      },
      "source": [
        "###3.2.1 Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Nwoeu2uKqFpB",
        "colab": {}
      },
      "source": [
        "# First, let's collect our data in one long text:\n",
        "long_text = []                        \n",
        "for speaker, text in protocol: \n",
        "    if \"zeug\" in speaker.lower():\n",
        "        long_text.append(text + \".\")\n",
        "long_text = \" \".join(long_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Fm44frZxqFpC",
        "colab": {}
      },
      "source": [
        "def remove_punctuation(text):\n",
        "    punctuation_mapping = {ord(c): None for c in string.punctuation}\n",
        "    return text.translate(punctuation_mapping)\n",
        "\n",
        "def sentence_tokenizer(text):\n",
        "    '''\n",
        "    Versy simple sentence tokenizer using the sequence '. ' to split. \n",
        "    However, as this sequence also appears in other contexts, we will\n",
        "    try to remove them first.\n",
        "    '''\n",
        "    mapping = {'bzw.':'bzw',\n",
        "               'Bzw.':'bzw',\n",
        "               'etc. pp.': 'etc pp',\n",
        "               'etc.':'etc',\n",
        "               'usw.':'usw',\n",
        "               'usf.':'usf',\n",
        "               'z.B.':'zum Beispiel',\n",
        "               'z. b.':'zum Beispiel',\n",
        "               'a. D.':'außer Dienst',\n",
        "               'Abs.':'abs',\n",
        "               'u. a.':'unter anderem',\n",
        "               'evtl.':'eventuell',\n",
        "               'ggf.':'gegebenenfalls',\n",
        "               'f.':'f',\n",
        "               'o. g.':'oben genannten',\n",
        "               'ca.':'ca',\n",
        "               'ff.':'ff'}\n",
        "\n",
        "    for k, v in mapping.items():\n",
        "        text = text.replace(k, v)\n",
        "\n",
        "    # remove dots in numbers specifically dates\n",
        "    # '21.3.' and '5. September' but not '1998.':\n",
        "    text = re.sub(r'([\\. ]\\d[\\d])\\. ', '\\\\1 ', text)\n",
        "    \n",
        "    # Some names are anonymized with a dot: (P., R., W. etc.)\n",
        "    text = re.sub(r'([A-Z])\\. ', '\\\\1 ', text)\n",
        "\n",
        "    return list(text.split('. '))\n",
        "\n",
        "# Now lets split into sentences:\n",
        "data = sentence_tokenizer(long_text)\n",
        "\n",
        "# And remove all punctuation and transform to lowercase. \n",
        "# Let's also remove very short sentences with less than 4 words:\n",
        "#data = [remove_punctuation(x.lower()) for x in data if len(x.split(' ')) > 4]\n",
        "data = [x for x in data if len(x.split(' '))> 4]\n",
        "\n",
        "print(len(data))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FUkEZgWrqFpF",
        "colab": {}
      },
      "source": [
        "# Now we need to label a bit of the data to create a training set for our text classification model:\n",
        "# Let's use our regular expression from before:\n",
        "    \n",
        "def cant_remember_matcher(text):\n",
        "      wildcards = r'[äöüßa-zÄÖÜA-Z- ]*'\n",
        "      # we only want to match relevant half sentences here to avoid confusions\n",
        "      # with negations in other parts of the same sentence:\n",
        "      halfsentence_start = r'(,|.|;|:|\\?|!|^)'\n",
        "      halfsentence_end = r'(,|.|;|:|\\?|!|$)'\n",
        "      bw = halfsentence_start + wildcards\n",
        "      wb = wildcards + halfsentence_end\n",
        "      sem_matcher = [f'{bw}keine{wildcards}Erinnerung{wb}',\n",
        "                     f'{bw}nicht{wildcards}Erinnerung{wb}',\n",
        "                     f'{bw}Erinnerung{wildcards}nicht{wb}',\n",
        "                     f'{bw}weiß{wildcards}nicht{wildcards}mehr{wb}',\n",
        "                     f'{bw}nicht{wildcards}erinner{wb}',\n",
        "                     f'{bw}erinner{wildcards}nicht{wb}',\n",
        "                     f'{bw}nicht{wildcards}entsinne{wb}',\n",
        "                     f'{bw}nicht{wildcards}Gedächtnis{wb}',\n",
        "                     f'{bw}Gedächtnis{wildcards}nicht{wb}',\n",
        "                     f'{bw}nicht{wildcards}mehr {wildcards}sagen{wb}',\n",
        "                     f'{bw}fällt{wildcards}nicht{wildcards}ein{wb}',\n",
        "                     f'{bw}nicht{wildcards}mehr{wildcards}rekonstruieren{wb}',\n",
        "                     f'{bw}nicht{wildcards}gegenwärtig{wb}',\n",
        "                     f'{bw}nicht{wildcards}präsent{wb}',\n",
        "                     f'{bw}nicht{wildcards}im Kopf{wb}',\n",
        "                     f'{bw}nicht{wildcards}im Hinterkopf{wb}']\n",
        "      result = re.findall('|'.join(sem_matcher), text, re.IGNORECASE)\n",
        "      return True if result else False\n",
        "\n",
        "def build_dataset(data):\n",
        "      df = []\n",
        "      for idx, text in enumerate(data):\n",
        "          if cant_remember_matcher(text):\n",
        "              label = '1'\n",
        "          elif idx < 2000:\n",
        "              label = '0'\n",
        "          else:\n",
        "              label = ''\n",
        "\n",
        "          df.append({'label':label,\n",
        "                     'text':text})\n",
        "      return pd.DataFrame(df)\n",
        "\n",
        "\n",
        "df = build_dataset(data)      \n",
        "print(df.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "68xTNdacqFpH",
        "colab": {}
      },
      "source": [
        "def split_dataframe_by_ratio(df, ratio=0.8):\n",
        "    df.reset_index(inplace=True, drop=True)\n",
        "    ratio_index = int(df.shape[0]*ratio)\n",
        "    return df[:ratio_index], df[ratio_index:]\n",
        "\n",
        "df_labeled = df[(df['label']==\"1\") | (df['label'] == \"0\")]\n",
        "# Lets shuffle it:\n",
        "df_labeled = df_labeled.sample(frac=1)\n",
        "\n",
        "# Now let's split this dataframe into three smaller ones:\n",
        "# train (80% of the labeled rows)\n",
        "# eval (20% of the labeld rows)\n",
        "train_df, eval_df = split_dataframe_by_ratio(df_labeled, 0.8)\n",
        "\n",
        "# predict (all the unlabeld rows)\n",
        "predict_df = df[df['label']=='']\n",
        "predict_df.reset_index(inplace=True, drop=True)\n",
        "\n",
        "print(train_df.shape)\n",
        "print(eval_df.shape)\n",
        "print(predict_df.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XZDHKGHKGFd4"
      },
      "source": [
        "###3.2.2 Fasttext"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nf_X8sQXGCxF",
        "colab": {}
      },
      "source": [
        "fasttext_folder = sem_folder / '02_fasttext'\n",
        "Path.mkdir(fasttext_folder, exist_ok=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "n-Jh21CdqFpK",
        "colab": {}
      },
      "source": [
        "# Let's save our datasets to disk:\n",
        "# I strongly recommend to go through the train and eval dataset and check if everything has been set\n",
        "# up correctly. We naively assumed that everything that does not match our simple regular expressions \n",
        "# could be labeld as 'anything else', so we defintely have some false negatives in that dataset.\n",
        "\n",
        "with open(fasttext_folder / 'train.txt', 'w') as outfile:\n",
        "    for _, row in train_df.iterrows():\n",
        "        outfile.write(f'__label__{row[\"label\"]} {row[\"text\"]}\\n')\n",
        "\n",
        "with open(fasttext_folder / 'eval.txt', 'w') as outfile:\n",
        "    for _, row in eval_df.iterrows():\n",
        "        outfile.write(f'__label__{row[\"label\"]} {row[\"text\"]}\\n')\n",
        "                \n",
        "with open(fasttext_folder / 'predict.txt', 'w') as outfile:\n",
        "    for _, row in predict_df.iterrows():\n",
        "        outfile.write(f'{row[\"text\"]}\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zua_-6eDqFpN",
        "colab": {}
      },
      "source": [
        "# Let's doownload, unzip and compile fasttext:\n",
        "os.chdir(fasttext_folder)\n",
        "!wget https://github.com/facebookresearch/fastText/archive/v0.2.0.zip\n",
        "!unzip v0.2.0.zip\n",
        "os.chdir('./fastText-0.2.0')\n",
        "# You will need a working c++ compiler to sucessfully build it. \n",
        "# See here for more info: https://fasttext.cc/docs/en/support.html\n",
        "!make\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AUSSc-kxqFpQ",
        "colab": {}
      },
      "source": [
        "# Let's train the model:\n",
        "!./fasttext supervised -input ../train.txt -output model -epoch 25 -lr 0.5 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6pfGBNPeqFpU",
        "colab": {}
      },
      "source": [
        "# Lets test the model:\n",
        "!./fasttext test model.bin ../eval.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Zb7GJHKPI5Sb"
      },
      "source": [
        "It seem like the model performs well, however, our labeled data is rather small and labeled with help to a regular expressions, so different expressions, which we want to find here, are not represented in our labeled data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qlx2Kl2tqFpX",
        "colab": {}
      },
      "source": [
        "!./fasttext predict-prob model.bin ../predict.txt > ../predict_result.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IS7o9SQ3qFpb",
        "colab": {}
      },
      "source": [
        "# Let's change back to our project directory\n",
        "os.chdir(\"../../..\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ky6I67OtqFpf",
        "colab": {}
      },
      "source": [
        "# Let's look at the results:\n",
        "result = pd.read_csv(fasttext_folder / 'predict_result.txt', sep=\" \", header=None)\n",
        "result['text'] = predict_df['text']\n",
        "\n",
        "# Subset label 1:\n",
        "result = result[result[0]=='__label__1']\n",
        "result = result.sort_values([1], ascending=[0])\n",
        "\n",
        "result.to_csv(results_folder / 'fasttext_result.csv', sep=\"\\t\", index=False, encoding='utf-8')\n",
        "files.download(results_folder / 'fasttext_result.csv')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KCTdbglZIUJJ"
      },
      "source": [
        "The model performs poorly, we have a lot of false positives. It seems like it does not understand the negation in relation to remembering, it identifies many sentences that just contain negations, particularyl the word 'nicht'. So at best, this model is only useful to prefilter the data. Yet it was able to capture some expressions using words that were not reflected in the training data, for example: \n",
        "\n",
        "*   'Mir ist der Name Haydt auch heute nicht mehr geläufig'\n",
        "*   'Darüber habe ich keine Kenntnis mehr'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2wBAvIT2qFpk"
      },
      "source": [
        "### 3.2.3 BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4FTwgckJqFpl",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/google-research/bert.git 03_semantic_matching/03_bert"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UOs4c4rUqFpn",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "# Let's download the pretrained multilingual model:\n",
        "!wget -P 03_semantic_matching/03_bert https://storage.googleapis.com/bert_models/2018_11_23/multi_cased_L-12_H-768_A-12.zip\n",
        "!unzip 03_semantic_matching/03_bert/multi_cased_L-12_H-768_A-12.zip -d 03_semantic_matching/03_bert/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vyAxa_UAqFpr",
        "colab": {}
      },
      "source": [
        "bert_folder = sem_folder / '03_bert'\n",
        "data_folder = bert_folder / 'data'\n",
        "output_folder = bert_folder / 'bert_output'\n",
        "Path.mkdir(bert_folder, exist_ok=True)\n",
        "Path.mkdir(data_folder, exist_ok=True)\n",
        "Path.mkdir(output_folder, exist_ok=True)\n",
        "\n",
        "#Prepare data:\n",
        "train_df = train_df.assign(alpha='a')\n",
        "train_df = train_df.reindex(['label','alpha','text'], axis=1)\n",
        "train_df.index.names = ['user_id']\n",
        "train_df.to_csv(data_folder / 'train.tsv', sep='\\t', index=True, header=False)\n",
        "\n",
        "eval_df = eval_df.assign(alpha='a')\n",
        "eval_df = eval_df.reindex(['label','alpha','text'], axis=1)\n",
        "eval_df.index.names = ['user_id']\n",
        "eval_df.to_csv(data_folder / 'dev.tsv', sep=\"\\t\", index=True, header=False)\n",
        "\n",
        "predict_df = predict_df.drop('label', axis=1)\n",
        "predict_df.to_csv(data_folder / 'test.tsv', sep='\\t', index=True, header=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDnFpCp4tmaQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 1.x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aPdYe0PzqFpt",
        "colab": {}
      },
      "source": [
        "# Let's fine-tune the bert model. This will take some time (41mins for me)\n",
        "%%time\n",
        "!python 03_semantic_matching/03_bert/run_classifier.py \\\n",
        "--task_name=cola \\\n",
        "--do_train=true \\\n",
        "--do_eval=true \\\n",
        "--do_predict=true \\\n",
        "--data_dir=03_semantic_matching/03_bert/data/ \\\n",
        "--vocab_file=03_semantic_matching/03_bert/multi_cased_L-12_H-768_A-12/vocab.txt \\\n",
        "--bert_config_file=03_semantic_matching/03_bert/multi_cased_L-12_H-768_A-12/bert_config.json \\\n",
        "--init_checkpoint=03_semantic_matching/03_bert/multi_cased_L-12_H-768_A-12/bert_model.ckpt \\\n",
        "--max_seq_length=400 \\\n",
        "--train_batch_size=6 \\\n",
        "--learning_rate=2e-5 \\\n",
        "--num_train_epochs=3.0 \\\n",
        "--output_dir=03_semantic_matching/03_bert/bert_output/ \\\n",
        "--do_lower_case=False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sVUeFSKC_IFa",
        "colab": {}
      },
      "source": [
        "!cat 03_semantic_matching/03_bert/bert_output/eval_results.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RJmCtg8R_Qq1",
        "colab": {}
      },
      "source": [
        "# Let's look at the results:\n",
        "result = pd.read_csv('03_semantic_matching/03_bert/bert_output/test_results.tsv', sep=\"\\t\", header=None)\n",
        "result['text'] = predict_df['text']\n",
        "\n",
        "# Sort by highest probability for label 1:\n",
        "result = result.sort_values(1, ascending=False)\n",
        "\n",
        "print(result.loc[result[1] > 0.9].shape)\n",
        "result.to_csv(results_folder / 'bert_result.tsv', sep=\"\\t\", index=False, encoding='utf-8')\n",
        "files.download(results_folder / 'bert_result.tsv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "H8INosD9aB_K",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}